{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umrsid/Cool-LLM-Projects-with-UI/blob/main/chatbot_with_vectorDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f972f82d",
      "metadata": {
        "id": "f972f82d"
      },
      "source": [
        "### Basic working of Google Palm LLM in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain google-generativeai faiss-cpu"
      ],
      "metadata": {
        "id": "COLkn_tws7t2"
      },
      "id": "COLkn_tws7t2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install InstructorEmbedding\n",
        "!pip install sentence_transformers\n",
        "import InstructorEmbedding"
      ],
      "metadata": {
        "id": "yuVp40J7spmK"
      },
      "id": "yuVp40J7spmK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34aa70b",
      "metadata": {
        "id": "a34aa70b"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import GooglePalm\n",
        "\n",
        "api_key = '//' # get this free api key from https://makersuite.google.com/\n",
        "\n",
        "llm = GooglePalm(google_api_key=api_key, temperature=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b610123",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b610123",
        "outputId": "4892c7dd-0a93-4a5e-f27d-aff2b14ccc37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Oh, samosa, my love**\n",
            "**You are so delicious, so flaky**\n",
            "**With your filling of potatoes and peas**\n",
            "**I could eat you every day**\n"
          ]
        }
      ],
      "source": [
        "poem = llm(\"Write a 4 line poem of my love for samosa\")\n",
        "print(poem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c235a80e",
      "metadata": {
        "scrolled": true,
        "id": "c235a80e"
      },
      "outputs": [],
      "source": [
        "essay = llm(\"write email requesting refund for electronic item\")\n",
        "print(essay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227816a9",
      "metadata": {
        "id": "227816a9"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.llms import GooglePalm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765695b5",
      "metadata": {
        "id": "765695b5"
      },
      "source": [
        "### Now let's load data from Codebasics FAQ csv file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"homo_deus_a_brief_history_of_tomorrow_pdf.pdf\")\n",
        "data = loader.load_and_split()\n"
      ],
      "metadata": {
        "id": "axWhqJTAuJsi"
      },
      "id": "axWhqJTAuJsi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[2]\n"
      ],
      "metadata": {
        "id": "jHkCnNzbvBPA",
        "outputId": "4f778bba-c21a-45f3-efda-38eb64a3bf28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jHkCnNzbvBPA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='In vitro\\n fertilisation: mastering creation.\\nComputer artwork ¬© KTSDESIGN/Science Photo Library.', metadata={'source': 'homo_deus_a_brief_history_of_tomorrow_pdf.pdf', 'page': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c62e35c",
      "metadata": {
        "id": "0c62e35c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# loader = CSVLoader(file_path='codebasics_faqs.csv', source_column=\"prompt\")\n",
        "\n",
        "# # Store the loaded data in the 'data' variable\n",
        "# data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd45e51",
      "metadata": {
        "id": "4dd45e51"
      },
      "source": [
        "### Hugging Face Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a4de8c",
      "metadata": {
        "id": "04a4de8c"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "# Initialize instructor embeddings using the Hugging Face model\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-large\")\n",
        "\n",
        "e = instructor_embeddings.embed_query(\"What is your refund policy?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0762eeac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0762eeac",
        "outputId": "ba3fd68b-b317-4a3f-e8cc-6b6f1fae857b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6fab6ce",
      "metadata": {
        "id": "e6fab6ce"
      },
      "outputs": [],
      "source": [
        "e[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e571c0d2",
      "metadata": {
        "id": "e571c0d2"
      },
      "source": [
        "As you can see above, embedding for a sentance \"What is your refund policy\" is a list of size 768. Looking at the numbers in this list, doesn't give any intuitive understanding of what it is but just assume that these numbers are capturing the meaning of \"What is your refund policy\". If you are curious to know about embeddings, go to youtube and search \"codebasics word embeddings\" and you will find bunch of videos with simple, intuitive explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc80a28a",
      "metadata": {
        "id": "cc80a28a"
      },
      "source": [
        "### Vector store using FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c706da",
      "metadata": {
        "id": "b3c706da"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Create a FAISS instance for vector database from 'data'\n",
        "vectordb = FAISS.from_documents(documents=[data[0]],\n",
        "                                 embedding=instructor_embeddings)\n",
        "\n",
        "# Create a retriever for querying the vector database\n",
        "retriever = vectordb.as_retriever(score_threshold = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd58f6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfd58f6f",
        "outputId": "bb1fe0e0-c2f9-4a90-8839-acbc37ea6418"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Dedication\\nTo my teacher, S. N. Goenka (1924‚Äì2013),\\nwho lovingly taught me important things.', metadata={'source': 'homo_deus_a_brief_history_of_tomorrow_pdf.pdf', 'page': 2})]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "rdocs = retriever.get_relevant_documents(\"how about job placement support?\")\n",
        "rdocs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf6b257",
      "metadata": {
        "id": "4cf6b257"
      },
      "source": [
        "As you can see above, the retriever that was created using FAISS and hugging face embedding is now capable of pulling relavant documents from our original CSV file knowledge store. This is very powerful and it will help us further in our project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45bee857",
      "metadata": {
        "id": "45bee857"
      },
      "source": [
        "##### Embeddings can be created using GooglePalm too. Also for vector database you can use chromadb as well as shown below. During our experimentation, we found hugging face embeddings and FAISS to be more appropriate for our use case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a93d079d",
      "metadata": {
        "id": "a93d079d"
      },
      "outputs": [],
      "source": [
        "# google_palm_embeddings = GooglePalmEmbeddings(google_api_key=api_key)\n",
        "\n",
        "# from langchain.vectorstores import Chroma\n",
        "# vectordb = Chroma.from_documents(data,\n",
        "#                            embedding=google_palm_embeddings,\n",
        "#                            persist_directory='./chromadb')\n",
        "# vectordb.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f3d927",
      "metadata": {
        "id": "02f3d927"
      },
      "source": [
        "### Create RetrievalQA chain along with prompt template üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d4842c8",
      "metadata": {
        "id": "2d4842c8"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
        "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
        "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
        "\n",
        "CONTEXT: {context}\n",
        "\n",
        "QUESTION: {question}\"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                            chain_type=\"stuff\",\n",
        "                            retriever=retriever,\n",
        "                            input_key=\"query\",\n",
        "                            return_source_documents=True,\n",
        "                            chain_type_kwargs=chain_type_kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "152a4cf8",
      "metadata": {
        "id": "152a4cf8"
      },
      "source": [
        "### We are all set üëçüèº Let's ask some questions now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90166e8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90166e8d",
        "outputId": "67515387-93b9-4c49-bbf7-4d297940c408"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'to whom this book is dedicated',\n",
              " 'result': 'To my teacher, S. N. Goenka (1924‚Äì2013),\\nwho lovingly taught me important things.',\n",
              " 'source_documents': [Document(page_content='Dedication\\nTo my teacher, S. N. Goenka (1924‚Äì2013),\\nwho lovingly taught me important things.', metadata={'source': 'homo_deus_a_brief_history_of_tomorrow_pdf.pdf', 'page': 2})]}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "chain('to whom this book is dedicated')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a4e3e4",
      "metadata": {
        "id": "b3a4e3e4"
      },
      "source": [
        "**As you can see above, the answer of question comes from two different FAQs within our csv file and it is able to pull those questions and merge them nicely**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82dce73e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82dce73e",
        "outputId": "f789fed3-799a-4c81-c99b-2b58465c2d60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Do you guys provide internship and also do you offer EMI payments?',\n",
              " 'result': \"I don't know.\",\n",
              " 'source_documents': [Document(page_content='Dedication\\nTo my teacher, S. N. Goenka (1924‚Äì2013),\\nwho lovingly taught me important things.', metadata={'source': 'homo_deus_a_brief_history_of_tomorrow_pdf.pdf', 'page': 2})]}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "chain(\"Do you guys provide internship and also do you offer EMI payments?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48970302",
      "metadata": {
        "scrolled": false,
        "id": "48970302"
      },
      "outputs": [],
      "source": [
        "chain(\"do you have javascript course?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c17dc6c3",
      "metadata": {
        "id": "c17dc6c3"
      },
      "outputs": [],
      "source": [
        "chain(\"Do you have plans to launch blockchain course in future?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c35c2c3",
      "metadata": {
        "id": "0c35c2c3"
      },
      "outputs": [],
      "source": [
        "chain(\"should I learn power bi or tableau?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a054c5ff",
      "metadata": {
        "id": "a054c5ff"
      },
      "outputs": [],
      "source": [
        "chain(\"I've a MAC computer. Can I use powerbi on it?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89fa5d10",
      "metadata": {
        "id": "89fa5d10"
      },
      "outputs": [],
      "source": [
        "chain(\"I don't see power pivot. how can I enable it?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6539e58",
      "metadata": {
        "id": "c6539e58"
      },
      "outputs": [],
      "source": [
        "chain(\"What is the price of your machine learning course?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}